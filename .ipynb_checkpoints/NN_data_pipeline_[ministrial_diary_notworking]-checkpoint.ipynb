{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd1cf936",
   "metadata": {},
   "source": [
    "# QLD Data Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5945080e",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "source": [
    "## Install and load Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77aa2407",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c3e170",
   "metadata": {},
   "source": [
    "- A correct version of chromedriver is required on the working directory. \\\n",
    "Please install the version that matches your browser.\n",
    "https://chromedriver.chromium.org/downloads\n",
    "\n",
    "- Ghostscript and ActiveTcl are required to be installed on the machine. \\\n",
    "See instructions on:\n",
    "https://camelot-py.readthedocs.io/en/master/user/install-deps.html \\\n",
    "Re-boot maybe required following a new installation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70896b2f",
   "metadata": {},
   "source": [
    "### Load Common Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3812643e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed Python version 3.8.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.display import display, HTML\n",
    "try:\n",
    "    import scrapy # scrape webpage\n",
    "except:\n",
    "    !pip install scrapy\n",
    "    import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "# text cleaning\n",
    "import re\n",
    "# Settings for notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "# Show Python version\n",
    "import platform\n",
    "print(\"Installed Python version\" , platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e1c1ba",
   "metadata": {},
   "source": [
    "## Extract & Transform\n",
    "Process all files into CSV and load CSVs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d345854",
   "metadata": {},
   "source": [
    "### Extract Lobbyist Data\n",
    "\n",
    "The Register of Lobbyists is a list of professional lobbyists who wish to lobby Government representatives. \\\n",
    "Extracting the QLD lobbysit data from https://lobbyists.integrity.qld.gov.au/register-details/list-companies.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df39b86b",
   "metadata": {},
   "source": [
    "#### middlewares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e655fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import signals\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import is_item, ItemAdapter\n",
    "\n",
    "\n",
    "class QldlobbyistSpiderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the spider middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_spider_input(self, response, spider):\n",
    "        # Called for each response that goes through the spider\n",
    "        # middleware and into the spider.\n",
    "\n",
    "        # Should return None or raise an exception.\n",
    "        return None\n",
    "\n",
    "    def process_spider_output(self, response, result, spider):\n",
    "        # Called with the results returned from the Spider, after\n",
    "        # it has processed the response.\n",
    "\n",
    "        # Must return an iterable of Request, or item objects.\n",
    "        for i in result:\n",
    "            yield i\n",
    "\n",
    "    def process_spider_exception(self, response, exception, spider):\n",
    "        # Called when a spider or process_spider_input() method\n",
    "        # (from other spider middleware) raises an exception.\n",
    "\n",
    "        # Should return either None or an iterable of Request or item objects.\n",
    "        pass\n",
    "\n",
    "    def process_start_requests(self, start_requests, spider):\n",
    "        # Called with the start requests of the spider, and works\n",
    "        # similarly to the process_spider_output() method, except\n",
    "        # that it doesnâ€™t have a response associated.\n",
    "\n",
    "        # Must return only requests (not items).\n",
    "        for r in start_requests:\n",
    "            yield r\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)\n",
    "\n",
    "\n",
    "class QldlobbyistDownloaderMiddleware:\n",
    "    # Not all methods need to be defined. If a method is not defined,\n",
    "    # scrapy acts as if the downloader middleware does not modify the\n",
    "    # passed objects.\n",
    "\n",
    "    @classmethod\n",
    "    def from_crawler(cls, crawler):\n",
    "        # This method is used by Scrapy to create your spiders.\n",
    "        s = cls()\n",
    "        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)\n",
    "        return s\n",
    "\n",
    "    def process_request(self, request, spider):\n",
    "        # Called for each request that goes through the downloader\n",
    "        # middleware.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this request\n",
    "        # - or return a Response object\n",
    "        # - or return a Request object\n",
    "        # - or raise IgnoreRequest: process_exception() methods of\n",
    "        #   installed downloader middleware will be called\n",
    "        return None\n",
    "\n",
    "    def process_response(self, request, response, spider):\n",
    "        # Called with the response returned from the downloader.\n",
    "\n",
    "        # Must either;\n",
    "        # - return a Response object\n",
    "        # - return a Request object\n",
    "        # - or raise IgnoreRequest\n",
    "        return response\n",
    "\n",
    "    def process_exception(self, request, exception, spider):\n",
    "        # Called when a download handler or a process_request()\n",
    "        # (from other downloader middleware) raises an exception.\n",
    "\n",
    "        # Must either:\n",
    "        # - return None: continue processing this exception\n",
    "        # - return a Response object: stops process_exception() chain\n",
    "        # - return a Request object: stops process_exception() chain\n",
    "        pass\n",
    "\n",
    "    def spider_opened(self, spider):\n",
    "        spider.logger.info('Spider opened: %s' % spider.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3206c8b",
   "metadata": {},
   "source": [
    "#### setup a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b1828b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itemadapter import ItemAdapter\n",
    "\n",
    "class QldlobbyistPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7579bbdb",
   "metadata": {},
   "source": [
    "#### model for scraped items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaff5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QldlobbyistItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b226e807",
   "metadata": {},
   "source": [
    "#### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577e09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from twisted.internet import reactor\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.log import configure_logging\n",
    "\n",
    "# Reactor restart\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "\n",
    "class QldlobbySpider(scrapy.Spider):\n",
    "    name = 'qldlobbyist'\n",
    "    allowed_domains = ['lobbyists.integrity.qld.gov.au']\n",
    "    start_urls = ['https://lobbyists.integrity.qld.gov.au/register-details/list-lobbyists.aspx']\n",
    "    custom_settings = {\n",
    "        'DOWNLOAD_DELAY': '0',\n",
    "        'BOT_NAME': 'qldlobbyist',\n",
    "        'SPIDER_MODULES': 'qldlobbyist.spiders',\n",
    "        'NEWSPIDER_MODULE': 'qldlobbyist.spiders',\n",
    "        'ROBOTSTXT_OBEY': 'False',\n",
    "        'FEEDS': {\n",
    "            'data/qldlobbyist.csv': { # csv output\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    def parse(self, response):\n",
    "            url =[]\n",
    "            endings = response.xpath('//*[@id=\"ListView\"]/li/a/@href')\n",
    "            for ending in endings:\n",
    "                url.append('https://lobbyists.integrity.qld.gov.au/register-details/'+ending.get())\n",
    "\n",
    "            for u in url:\n",
    "                yield scrapy.Request(url=u, callback = self.parse_client_data)\n",
    "    \n",
    "    def parse_client_data(self, response):\n",
    "        yield {\n",
    "            'Lobbyist Name' : response.xpath('//*[@id=\"ctl00_ContentPlaceholder1_lblName\"]/text()').get(),\n",
    "            'Lobbying Firm' : response.xpath('//*[@id=\"article\"]/div/table/tr[2]/td[2]//text()').get(),\n",
    "            'ABN' : response.xpath('//*[@id=\"article\"]/div/table/tr[3]/td[2]//text()').get(),\n",
    "            'Position' : response.xpath('//*[@id=\"article\"]/div/table/tr[4]/td[2]//text()').get(),\n",
    "            }\n",
    "        \n",
    "def run_spider():\n",
    "    \"\"\"run spider with qldlobbyist\"\"\"\n",
    "    crawler = CrawlerProcess()\n",
    "    d = crawler.crawl(QldlobbySpider)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2ef3da",
   "metadata": {},
   "source": [
    "#### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9465d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f0e262",
   "metadata": {},
   "source": [
    "#### Read the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9c7fe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lobbyist = pd.read_csv('data/qldlobbyist.csv')\n",
    "lobbyist.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11efb2aa",
   "metadata": {},
   "source": [
    "### Extract Lobbyist Client Data\n",
    "\n",
    "This data consists of third party clients of the Lobbyist that currently retain the services of the business to provide paid or unpaid lobbyist services. \\\n",
    "Extracting the QLD lobbysit clients data from https://lobbyists.integrity.qld.gov.au/register-details/list-clients.aspx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96696b4",
   "metadata": {},
   "source": [
    "####  The process for scraping the lobbyist client data uses the existing settings, and classes from the lobbyist data scraping process"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402bda8f",
   "metadata": {},
   "source": [
    "#### Define the spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c547b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Reactor restart\n",
    "from crochet import setup, wait_for\n",
    "setup()\n",
    "\n",
    "class QldlobClientSpider(scrapy.Spider):\n",
    "    name = 'qldlobbyclient'\n",
    "    allowed_domains = ['lobbyists.integrity.qld.gov.au']\n",
    "    start_urls = ['https://lobbyists.integrity.qld.gov.au/register-details/list-clients.aspx']\n",
    "    custom_settings = {\n",
    "                'DOWNLOAD_DELAY': '0',\n",
    "        'BOT_NAME': 'qldlobbyist',\n",
    "        'SPIDER_MODULES': 'qldlobbyist.spiders',\n",
    "        'NEWSPIDER_MODULE': 'qldlobbyist.spiders',\n",
    "        'ROBOTSTXT_OBEY': 'False',\n",
    "        'FEEDS': {\n",
    "            'data/qldlbClient.csv': { # csv output\n",
    "                'format': 'csv',\n",
    "                'overwrite': True\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    def parse(self, response):\n",
    "            url =[]\n",
    "            endings = response.xpath('//*[@id=\"ListView\"]/li/a/@href')\n",
    "            for ending in endings:\n",
    "                url.append('https://lobbyists.integrity.qld.gov.au/register-details/'+ending.get())\n",
    "\n",
    "            for u in url:\n",
    "                yield scrapy.Request(url=u, callback = self.parse_client_data)\n",
    "    \n",
    "    def parse_client_data(self, response):\n",
    "        yield {\n",
    "            'Client Name' : response.xpath('//*[@id=\"ctl00_ContentPlaceholder1_lblCName\"]/text()').get(),\n",
    "            'Lobbyist' : response.xpath('//*[@id=\"article\"]/div/table/tr[2]/td[2]//text()').get(),\n",
    "            'ABN' : response.xpath('//*[@id=\"article\"]/div/table/tr[3]/td[2]//text()').get(),\n",
    "            }\n",
    "\n",
    "def run_spider():\n",
    "    \"\"\"run spider with qldlobbyclient\"\"\"\n",
    "    crawler = CrawlerProcess()\n",
    "    d = crawler.crawl(QldlobClientSpider)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020f0589",
   "metadata": {},
   "source": [
    "#### Start the crawler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d999335",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "run_spider()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a89db8",
   "metadata": {},
   "source": [
    "#### Read the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46aa9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = pd.read_csv('data/qldlbClient.csv')\n",
    "client.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588a2762",
   "metadata": {},
   "source": [
    "### Extract Donations Data\n",
    "\n",
    "A disclosure return is the reporting of all donations, loans, and expenditure incurred for an election campaign. \n",
    "These must be reported to the ECQ under the Electoral Act 1992 and the Local Government Electoral Act 2011. \n",
    "All disclosures are made through the Electronic Disclosure System (EDS) and are available to the public.\n",
    "\n",
    "This data is scraped from the EDS (https://disclosures.ecq.qld.gov.au/Map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24411481-697b-45b9-8822-8c8129507260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import re\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "# Define the browser to use and then open up the donations homepage\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://disclosures.ecq.qld.gov.au/Map')\n",
    "\n",
    "# Empty List to collect new urls\n",
    "recipient_url = []\n",
    "\n",
    "# Read the latest donations csv and determine latest date\n",
    "df_date = pd.DataFrame()\n",
    "df_donations = pd.read_csv('data/qld_donations.csv')\n",
    "df_date['date'] = df_donations.date.dropna()\n",
    "df_date['date'] = pd.to_datetime(df_date['date'], format='%d/%m/%Y')\n",
    "last_date = max(df_date.date)\n",
    "newest_date = (last_date.date().strftime('%d-%m-%Y'))\n",
    "\n",
    "# Input the last date into the website, then apply filter\n",
    "date_from = driver.find_element(By.XPATH, \"//*[@id='ViewFilter_DateFrom']\")\n",
    "date_from.send_keys(newest_date)\n",
    "# date_from.send_keys('01-09-2022') # for manual filter\n",
    "apply_button = driver.find_element(By.XPATH, \"//*[@id='maps-form']/div/div/div[2]/div[1]/button\")\n",
    "apply_button.click()\n",
    "time.sleep(2)\n",
    "\n",
    "# 2. Begin scraping the urls\n",
    "while True:\n",
    "    \n",
    "     # This code selects all the URLs from the main table, then appends URLs to a list\n",
    "    recip_urls = driver.find_elements(By.XPATH, \"//tr/td[3]/a\")\n",
    "    for url in recip_urls:\n",
    "        #print(url.get_attribute(\"href\"))\n",
    "        recipient_url.append(url.get_attribute(\"href\"))\n",
    "    \n",
    "    # for current page, grab the page items at bottom\n",
    "    page_items_element = driver.find_element(By.XPATH, \"//*[@id='map-page-header']/div[2]/div[3]/div/div/div/div/div/div[2]/small\")\n",
    "    page_items = page_items_element.text\n",
    "    page_item_nos = re.findall(r'\\d+', page_items)\n",
    "    \n",
    "    # pagination if page items are not at the end\n",
    "    if page_item_nos[1] != page_item_nos[2]:\n",
    "        # looks for the next button\n",
    "        button = driver.find_element(By.CLASS_NAME, \"fa.fa-chevron-right\")\n",
    "        button.click()\n",
    "        time.sleep(2)\n",
    "    \n",
    "    else:\n",
    "        # exits when the button no longer exists\n",
    "        break\n",
    "\n",
    "# Instantiate all lists        \n",
    "all_urls = recipient_url\n",
    "date = []\n",
    "donor_name = []\n",
    "donor_type = []\n",
    "recipient_name = []\n",
    "recipient_type = []\n",
    "agent_names = []\n",
    "gift_type = []\n",
    "gift_value = []\n",
    "page_url= []\n",
    "\n",
    "\n",
    "# Scrape the pages from each of the just scraped urls\n",
    "for url in all_urls:\n",
    "    driver.get(url)\n",
    "    \n",
    "    # Donor\n",
    "    try: \n",
    "        driver.find_element(By.XPATH, \"//*[@class='form-group']/div/input\")\n",
    "    except NoSuchElementException:\n",
    "        donor_name.append(None)\n",
    "    else:\n",
    "        donor_dim = driver.find_element(By.XPATH, \"//*[@class='form-group']/div/input\")\n",
    "        donor_name.append(donor_dim.get_attribute(\"value\"))\n",
    "    \n",
    "    \n",
    "    # Donor_Types\n",
    "    try: \n",
    "        driver.find_element(By.XPATH, \"//*[@id='disclosureEntries']/div/div/div[1]/div/span[4]\")\n",
    "    except NoSuchElementException:\n",
    "        donor_type.append(None)\n",
    "    else:\n",
    "        donor_type_dim = driver.find_element(By.XPATH, \"//*[@id='disclosureEntries']/div/div/div[1]/div/span[4]\")\n",
    "        donor_type.append(donor_type_dim.text)\n",
    "\n",
    "    # Recipient\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//*[@id='Head_ElectorFullName']\")\n",
    "    except NoSuchElementException:\n",
    "        try:\n",
    "            driver.find_element(By.XPATH, \"//*[@id='Head_PoliticalPartyTitle']\")\n",
    "        except NoSuchElementException:\n",
    "            try:\n",
    "                driver.find_element(By.XPATH, \"//*[@id='Head_Title']\")\n",
    "            except NoSuchElementException:\n",
    "                recipient_name.append(None)\n",
    "            else:\n",
    "                recipient_dim = driver.find_element(By.XPATH, \"//*[@id='Head_Title']\")\n",
    "                recipient_name.append(recipient_dim.get_attribute(\"value\"))\n",
    "        else:\n",
    "            recipient_dim = driver.find_element(By.XPATH, \"//*[@id='Head_PoliticalPartyTitle']\")\n",
    "            recipient_name.append(recipient_dim.get_attribute(\"value\"))\n",
    "    else:\n",
    "        recipient_dim = driver.find_element(By.XPATH, \"//*[@id='Head_ElectorFullName']\")\n",
    "        recipient_name.append(recipient_dim.get_attribute(\"value\"))    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Recipient Type\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//*[@id='content']/div/div[1]/div/div/div/div/div[1]/h1\")\n",
    "    except NoSuchElementException:\n",
    "        recipient_type.append(None)\n",
    "    else:\n",
    "        recip_type_dim = driver.find_element(By.XPATH, \"//*[@id='content']/div/div[1]/div/div/div/div/div[1]/h1\")\n",
    "        recipient_type.append(recip_type_dim.text)\n",
    "    \n",
    "    # Date\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//*[@class='form-control datepicker gift-date-received special-reporting-event-check-trigger']\")\n",
    "    except NoSuchElementException:\n",
    "        date.append(None)\n",
    "    else:\n",
    "        date_dim = driver.find_element(By.XPATH, \"//*[@class='form-control datepicker gift-date-received special-reporting-event-check-trigger']\")\n",
    "        date.append(date_dim.get_attribute(\"value\"))\n",
    "    \n",
    "    # Gift value\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//*[@class='form-control currencyFormat text-right gift-amount special-reporting-event-check-trigger']\")\n",
    "    except NoSuchElementException:\n",
    "        gift_value.append(None)\n",
    "    else:\n",
    "        gift_value_dim = driver.find_element(By.XPATH, \"//*[@class='form-control currencyFormat text-right gift-amount special-reporting-event-check-trigger']\")\n",
    "        gift_value.append(gift_value_dim.get_attribute(\"value\"))\n",
    "    \n",
    "    # Gift Type\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//*[@id='disclosureEntries']/div/div/div[1]/div/span[2]\")\n",
    "    except NoSuchElementException:\n",
    "        gift_type.append(None)\n",
    "    else:\n",
    "        gift_type_dim = driver.find_element(By.XPATH, \"//*[@id='disclosureEntries']/div/div/div[1]/div/span[2]\")\n",
    "        gift_type.append(gift_type_dim.text)\n",
    "    \n",
    "    # Agent Names\n",
    "    try:\n",
    "        driver.find_element(By.XPATH, \"//*[@id='Head_RepresentativeFullName']\")\n",
    "    except NoSuchElementException:\n",
    "        try: \n",
    "            driver.find_element(By.XPATH, \"//*[@id='Head_AgentFullName']\")\n",
    "        except NoSuchElementException:\n",
    "            agent_names.append(None)\n",
    "        else:\n",
    "            agent_name = driver.find_element(By.XPATH, \"//*[@id='Head_AgentFullName']\")\n",
    "            agent_names.append(agent_name.get_attribute(\"value\"))   \n",
    "    else:\n",
    "        agent_name = driver.find_element(By.XPATH, \"//*[@id='Head_RepresentativeFullName']\")\n",
    "        agent_names.append(agent_name.get_attribute(\"value\"))\n",
    "        \n",
    "        \n",
    "    # original url\n",
    "    page_url.append(driver.current_url)\n",
    "        \n",
    "    time.sleep(0.5)\n",
    "    \n",
    "    \n",
    "# Create dataframe for the newly scraped pages\n",
    "df = pd.DataFrame(list(zip(date, donor_name, donor_type, recipient_name, recipient_type, agent_names, gift_type, gift_value, page_url)), \n",
    "                  columns = [\"date\", \"donor_name\", \"donor_type\", \"recipient_name\", \"recipient_type\", \"agent_names\", \"gift_type\", \"gift_value\", \"page_url\"])\n",
    "\n",
    "# Clean any whitespaces\n",
    "df['recipient_name'] = df['recipient_name'].str.strip()\n",
    "df['donor_name'] = df['donor_name'].str.strip()\n",
    "df['agent_names'] = df['agent_names'].str.strip()\n",
    "\n",
    "# Import existing data, concatenate the new data with existing dataset, remove duplicates\n",
    "df_concat = pd.concat([df, df_donations], ignore_index=True)\n",
    "df_dupes_dropped = df_concat.drop_duplicates(subset=['page_url'])\n",
    "\n",
    "# Save new file\n",
    "df_dupes_dropped.to_csv('data/qld_donations.csv', header=True, index=False)\n",
    "\n",
    "# close the browser\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e34d0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "donations = df_dupes_dropped\n",
    "donations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d23457",
   "metadata": {},
   "source": [
    "### Extract Ministerial Diaries Data\n",
    "\n",
    "Ministers are required to proactively disclose on a monthly basis portfolio related meetings and events.\n",
    "\n",
    "For any meeting with a registered lobbyist or any person working for the lobbyist in any capacity, other than administrative staff, the diary must also include details about all attendees and a short description of the subject matter of the meeting.\n",
    "\n",
    "Personal, electorate or party political meetings or events, media events and interviews and information contrary to public interest (e.g. meetings regarding sensitive law enforcement, public safety or whistle-blower matters) are not to be released.\n",
    "\n",
    "This data is scraped from https://cabinet.qld.gov.au/ministers-portfolios.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "381693db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import camelot\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c958e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ctypes\n",
    "from ctypes.util import find_library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5a30a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Program Files\\\\gs\\\\gs10.00.0\\\\bin\\\\gsdll64.dll'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This checks if Ghostscript is installed correctly. This shouldn't return blank.\n",
    "find_library(\"\".join((\"gsdll\", str(ctypes.sizeof(ctypes.c_voidp) * 8), \".dll\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f4614b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set domain\n",
    "QLD = \"https://cabinet.qld.gov.au/ministers-portfolios.aspx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce2ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape links for ministrial diaries (pdf download links)\n",
    "page = requests.get(QLD)\n",
    "soup = bs(page.content, \"html.parser\")\n",
    "results = soup.find_all(\"a\")\n",
    "\n",
    "urls = []\n",
    "names = []\n",
    "error_files = []\n",
    "d = {}\n",
    "for i, link in enumerate(results):\n",
    "    current_link = link.get('href')\n",
    "    if 'ministers-portfolios/' in current_link:\n",
    "        \n",
    "        current_link = current_link.split(\"/\")[-1].split(\".\")[0]\n",
    "        #print(current_link)\n",
    "        \n",
    "       \n",
    "        minister_page = requests.get(\"https://cabinet.qld.gov.au/ministers-portfolios/\"+current_link+\".aspx\")\n",
    "        \n",
    "        minister_soup = bs(minister_page.content, \"html.parser\")\n",
    "        minister_results = minister_soup.find_all(\"a\")\n",
    "            \n",
    "        \n",
    "        for l, minister_link in enumerate(minister_results):\n",
    "            current_link1 = minister_link.get('href')\n",
    "            \n",
    "            if current_link1.endswith('pdf'):\n",
    "                current_link1 = current_link1.replace(\"/ministers-portfolios/\", \"\")\n",
    "                urls.append(current_link1)\n",
    "                names.append(current_link)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e17af85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store download pdfs\n",
    "names_urls = zip(names, urls)\n",
    "\n",
    "errorURL= []\n",
    "\n",
    "for name, url in names_urls:\n",
    "    if \"http://\" not in url:\n",
    "        try: \n",
    "            if \"charter-letter\" not in url:\n",
    "                year, month = url.split(\"/\")[2], url.split(\"/\")[3]\n",
    "            else:\n",
    "                month = \"charter\"\n",
    "                year = \"-letter\"\n",
    "            r = requests.get(\"https://cabinet.qld.gov.au/ministers-portfolios/\"+url)\n",
    "            with open(\"qld_min_diaries//\" + name+\"_\"+month+year+\".pdf\", \"wb\") as f:\n",
    "                f.write(r.content)\n",
    "        except: \n",
    "            errorURL.append(url)\n",
    "            pass\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b54bde",
   "metadata": {},
   "source": [
    "#### Extract from pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c56273a5",
   "metadata": {},
   "source": [
    "##### Function to get minister's name from the file name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "59bb8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_portfolio(file_name):\n",
    "    Portfolio_temp =file_name.split(\"_\") # changed from - to _ to get full names\n",
    "    cmonth =  ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December']\n",
    "    Portfolio = \"\"\n",
    "    for prt in Portfolio_temp:\n",
    "        if isinstance(prt, int) or prt in cmonth or \"pdf\" in prt:\n",
    "            pass\n",
    "        else:\n",
    "            Portfolio += prt +\" \"\n",
    "    return Portfolio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12851a33",
   "metadata": {},
   "source": [
    "##### Function to get table from the pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6d70493b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_tables(file, portfolio):\n",
    "    tables = camelot.read_pdf(file, pages='all')\n",
    "\n",
    "    for table in tables:\n",
    "        for idx, row in table.df.iterrows():\n",
    "            if row[0] == \"\":\n",
    "                date = next_date\n",
    "            else:\n",
    "                date = row[0]\n",
    "            \n",
    "            \n",
    "            purpose = row[2]\n",
    "            if purpose == \"\":\n",
    "                    purpose = next_purpose  #if the next page does not have date or purpose take it from previous page\n",
    "                    \n",
    "            if \"Date\" not in row[0]:\n",
    "                orgs = row[1].split(\"\\n\")\n",
    "                next_date = date\n",
    "                next_purpose = purpose #if the next page does not have date or purpose take it from previous page\n",
    "                for org in orgs:\n",
    "                    org = org.replace(\"Â¬â€ \", \" \")\n",
    "                    df[\"Date\"].append(date)\n",
    "                    df[\"Ministers\"].append(portfolio)\n",
    "                    df[\"Organisation/Individual\"].append(org)\n",
    "                    df[\"Purpose of Meeting\"].append(purpose)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c2f03bc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Ministers</th>\n",
       "      <th>Organisation/Individual</th>\n",
       "      <th>Purpose of Meeting</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Date, Ministers, Organisation/Individual, Purpose of Meeting]\n",
       "Index: []"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diaries = { \"Date\":[], \"Ministers\":[], \"Organisation/Individual\": [], \"Purpose of Meeting\":[]}\n",
    "# exclude letter.pdf\n",
    "pdf_files = list(set(glob.glob(\"qld_min_diaries/*.pdf\"))-set(glob.glob(\"qld_min_diaries/*letter.pdf\")))\n",
    "err_files = []\n",
    "for file in pdf_files:\n",
    "    file_name = file[len(\"qld_min_diaries/\"):]\n",
    "    portfolio = get_portfolio(file_name)\n",
    "    try:\n",
    "        extract_tables(file, portfolio)\n",
    "    except:\n",
    "        err_files.append(file_name)\n",
    "        pass\n",
    "\n",
    "diaries = pd.DataFrame(diaries)\n",
    "diaries.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4d2b1ab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diaries.to_csv(\"data/qld_diaries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96aa98e1",
   "metadata": {},
   "source": [
    "# Pre-processing\n",
    "To extract 'source', 'target', and 'weight' columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a47986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lobbyist\n",
    "ag_lobbyist = lobbyist.groupby(['Lobbying Firm','Lobbyist Name']).size().reset_index(name='weight')\n",
    "ag_lobbyist = ag_lobbyist.rename(columns={'Lobbying Firm': 'source', 'Lobbyist Name': 'target'})\n",
    "ag_lobbyist['color'] = '#9b5de5'\n",
    "ag_lobbyist['group'] = 'Lobbyist'\n",
    "ag_lobbyist.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2503610",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client\n",
    "ag_client = client.groupby(['Lobbyist','Client Name']).size().reset_index(name='weight')\n",
    "ag_client = ag_client.rename(columns={'Lobbyist': 'source', 'Client Name': 'target'})\n",
    "ag_client['color'] = '#fee440'\n",
    "ag_client['group'] = 'Client'\n",
    "ag_client.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f72cb491",
   "metadata": {},
   "outputs": [],
   "source": [
    "# donations\n",
    "ag_donations = donations.groupby(['Donor','Recipient']).size().reset_index(name='weight')\n",
    "ag_donations = ag_donations.rename(columns={'Donor': 'source', 'Recipient': 'target'})\n",
    "ag_donations['color'] = '#00bbf9'\n",
    "ag_donations['group'] = 'Donnation'\n",
    "ag_donations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e9c0caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# donations - with amount\n",
    "ag_donations_sum = donations.groupby(['Donor','Recipient'])['Gift value'].agg(['sum', 'count']).reset_index()\n",
    "ag_donations_sum = ag_donations_sum.rename(columns={'Donor': 'source', 'Recipient': 'target', 'sum':'amount', 'count':'weight'})\n",
    "ag_donations_sum.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0eba473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# diaries\n",
    "# client\n",
    "ag_diaries = diaries.groupby(['Party','Name']).size().reset_index(name='weight')\n",
    "ag_diaries = ag_diaries.rename(columns={'Party': 'source', 'Name': 'target'})\n",
    "ag_diaries['color'] = '#00f5d4'\n",
    "ag_diaries['group'] = 'Ministers'\n",
    "ag_diaries.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e0c2e6",
   "metadata": {},
   "source": [
    "# Stack all data sets\n",
    "map_dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec456992",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_dat = pd.concat([ag_lobbyist,ag_client,ag_donations,ag_diaries],ignore_index=True)\n",
    "#map_dat.to_csv(r'data\\map_dat.csv', encoding='utf-8', index=False)\n",
    "\n",
    "map_dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91182b75-e3a9-4a2c-bfa6-1dd371497c0f",
   "metadata": {},
   "source": [
    "## Different Algorithms for Network\n",
    "barnes_hut: based on vectorisation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d84120-2331-4db5-b1f4-e887a63e5e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_algs(g, alg=\"barnes\"): #setting default algorithm to barnes_hut\n",
    "    if alg==\"barnes\":\n",
    "        g.barnes_hut()\n",
    "    if alg==\"forced\":\n",
    "        g.forced_atlas_2based()\n",
    "    if alg==\"hrepulsion\":\n",
    "        g.hrepulsion()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8e86cb-0d12-4fc7-a49d-1b7cc0e22454",
   "metadata": {},
   "source": [
    "## Available shapes for nodes:\n",
    " LABEL INSIDE: ellipse, cirlce, database, box, text\n",
    " LABEL OUTSIDE: image, circularImage, diamond, dot, star, triangle, triangleDown, square, icon\n",
    " size(num(optional)), only for nodes with outside labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df79be3-83eb-4151-bde4-1e77a65fb855",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_data(map_dat, \n",
    "            src_color=\"#03DAC6\", tgt_color=\"#da03b3\", edge_color=\"#018786\",\n",
    "            src_shape=\"cirlce\", tgt_shape=\"cirlce\",\n",
    "            algorithm=\"barnes\",\n",
    "            buttons=False):\n",
    "    g = Network(height = '1000px', width = '100%', bgcolor='#222222', font_color='white', directed=False)\n",
    "    \n",
    "    #create a node for each donor and recipient\n",
    "    if buttons==True:\n",
    "        g.width=\"75%\"\n",
    "        g.show_buttons(filter_=[\"physics\", \"edges\", \"nodes\", \"layout\"])\n",
    "        # physics button might be most useful for users, real time modification of node behaviour\n",
    "        # the different button panels are nodes, layout, interaction, manipulation, selection, renderer, physics\n",
    "        \n",
    "    #Dynamic edges can make it so we can see multiple interactions between nodes\n",
    "        #width and smooth options some of the most useful ones\n",
    "    \n",
    "    #Changing node sizes, can only change those with lables on the outside.\n",
    "        #Can change shadow, border, size, etc\n",
    "        \n",
    "    #Interaction panel:\n",
    "        #Can enable navigation buttons\n",
    "        #Multiselect options\n",
    "        #Hover will allow you to highlight edges/nodes by hovering\n",
    "        \n",
    "    #Copy and paste javascript into HTML file with all of the specific options selected\n",
    "    \n",
    "    #set your data\n",
    "    source = map_dat['source']\n",
    "    target = map_dat['target']\n",
    "    weight = map_dat['weight']\n",
    "    edge_data = zip(source, target, weight)\n",
    "    \n",
    "    for e in edge_data:\n",
    "        src = e[0]\n",
    "        tgt = e[1]\n",
    "        wgt = e[2]\n",
    "        g.add_node(src, src, title=src, color=src_color, shape=src_shape)\n",
    "        g.add_node(tgt, tgt, title=tgt, color=tgt_color, shape=tgt_shape)\n",
    "        g.add_edge(src, tgt, value=wgt, color=edge_color)\n",
    "    \n",
    "    #sets algorithm\n",
    "    map_algs(g, alg=algorithm)\n",
    "    \n",
    "    #setting the edges as dynamic\n",
    "    g.set_edge_smooth(\"dynamic\") #should see MULTIPLE facets of edges between nodes. can make different edges different colours\n",
    "    g.show(\"map_network_v1.0.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a28d991e-0efa-4fb4-8f00-49afbae3cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_data(map_dat, buttons=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9ebc98",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "c2ed1c1e3dda2d1d09010f3df351d528d97a393444344645f0fddd23da56b409"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
